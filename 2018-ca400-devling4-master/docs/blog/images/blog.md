# Blog: Car Deal Spotter

**Glen Devlin**

## Web-Scraping Cars Ireland - January, April 2018
I have scraped the initial data I will need to start building a predictive algorithm. I have scraped over 10,000 car ads, comprising 9 models each with enough data to allow me to perform effective analysis. The makes include Audi, BMW, Ford, Nissan, Skoda, Opel and Volkswagon. The scraper was built using python and makes use of the Beautiful Soup and the Requests libraries. Requests allows for the connection of the program to a web page and Beautiful Soup creates an object made of the pages HTML. At the moment I am using an SQLite database to store the data. The next step is to clean the data as there is some missing attributes. 
I returned to my web-scraper to fix problems that I had earlier. It will now only write the information to the appropriate cells in the database table. I increased the amount of information it retrieves. It now also gets the car’s County and various features relating to the speed and power of the cars. I do not know yet If I will use these in the model but they are better to have. On further inspection it seems that half of the cars share ids with cars I already have in my database meaning that out of the 10,000 cars I scraped, only half of them are new.

## Data Cleaning - January, February 2018
To clean the cars, I am using a combination of python and SQL. Although I initially used a lot of python, as I started to feel more comfortable and learn more with SQLite, I began to use that much more. The cars first few features such as make, model, price and year were always present or at least always had a value. I removed cars that were listed as POA or did not have a price as these were of no use to me. Some of the cars years were written in as ‘2017 (171)’ or ‘2017 (172)’, these would become 2017 and 2017.5 respectively. From the price and road tax I removed the euro sign. From the odometer I get only the number that represented the miles and dropped the kilometre equivalent. The Engine size column was trimmed so entries such as ‘4.0 and above’ became 4.0. The longest part of the cleaning was the generalisation of the colours. Although most colours were in as green, red, blue etc., a lot had strange names and these needed to be examined to convert them to 14 base colours. When scraping with my initial scraper I made a mistake that caused the data to be placed in the wrong column. To fix this I built a python function that would loop through the features, finding the most likely column for each feature. This only happened if the feature was deemed to be in the wrong column.
After the features were cleaned, missing values had to be filled so I could use this data as training data. To fill cells such as odometer I used the average of cars that were of a similar year and the same make. To fill the other rows, I mostly tried to find cars that were similar and use the most common occurrence. These functions can be viewed in the Data_Cleaner.py script. As I went down the features I found that features became missing as I went down the list. With road tax and fuel economy missing far more often than the other features. 

## Databases - January 2018, April 2018
When data is scraped, it is stored in a SQLite database on my home machine. They are saved as .db files and I use a database browser for SQLite to let me view the data and execute SQLite commands. The tables I used are training_cars(this is for the training data, where all cars eventually and up), display_cars(these are cars I plan to store for the user to browse, cars that are currently for sale) and scraped_cars(cars that have just been scraped by the scraper). 
The front-end application however will need a database that is stored online. For this reason, I use a hosting site called pythonanywhere.com. This site allows me to easily run python scripts online and provides options for a MySQL or a Postgres database. I chose a MySQL database. On this database I store my training cars and my display cars. The training cars to allow the user to value a car and display cars are so users can browse through cars that are for sale. I push the data from my home machine using python programs that connect to a flask python script I use as an api. I send my data through a URL, with the variables stored in the URL, once the page is loaded the row is stored in the database. 

## Predictive Algorithm - March, April 2018
To make the predictions on the car data I am using the python machine learning library sci-kit learn. Learning to do this has been the greatest challenge of the project. I had to learn to understand the regression model and how it was implemented in sci-kit. To implement sci-kit you also need two other libraries, Numpy and Pandas. Numpy is needed because it gives you access to arrays and Pandas are needed to form your data into a data frame. In a data frame your data is displayed in the same table form that it is in the database, except now it can be fitted to a model. The algorithm reads in data from the training dataset, selects the relevant cars, turns the data into a data frame and then fits the model. It writes a predicted price back to the column in the training set. Models are made only on cars of the same model. For example, the data for a model for an Audi A4 is made up of just other A4s. I can then find the corresponding car in the display cars using the cars ID and set the predicted price there. I also set a price difference. Categorical values must be encoded as numbers in order to be used. I have implemented a number of functions along with the predictive algorithm to help to gauge its performance. These functions perform tasks such as generating heatmaps to show the correlation between features. 

## Back-end - April, May 2018
I examined a few alternatives when looking for a place to build a back-end for my android application. I narrowed it down to and used the site, pythonanywhere.com. This is a hosting site that gives you access to MySQL databases and a platform to run python programs, this is exactly what I need. Setting up the databases was very easy through the use of the sites consoles. Originally, I was using SSH as a means of connecting to the database and uploading data. This was before I learned how to use Flask. By building a straightforward web app using flask I am able to make a python program containing functions, which all had addresses. By using the requests library to connect to these functions I am able to send them data, sending the cars to an online database. Pythonanywhere also allowed me to install sci-kit so I could use the programs I had already made online.  

## Front-end - April, May 2018
The front-end of my app is an android application. I am building it in Android Studio and have bought an Android phone for the purpose of demonstrating and testing. The app will allow users to browse cars, value cars and set alerts. To allow users to view multiple cars at once I am using a recycler view. This is a list widget that lets me use custom layouts. Users can enter search parameters on the first screen then are shown cars that match these parameter on the second screen. I was having trouble receiving a lot of data at once. To combat this, I searched for each car by the year retrieving the data piece by piece. So now when users are viewing cars they see them ordered by the year. To get data from the site I use the java URL class and readers to read the flask app returns. 
The value car feature works by getting users to fill in a form, once this form is filled in the users hit search and this sends the car features to the flask web app I made which contains the code to build the model. I can then read the return predicted price from and display it to the user. 

## Testing - May 2018
User testing for the front-end has shown me that the app is not as intuitive to use as I thought it would be. Users felt that the dropdown menus not being labelled was confusing. So, I have decided to move the labels outside of the boxes to sit alongside them. I also need to add more details that give hints or directions as it seems no user realised that each item in the recyclerview was clickable. Testing also helped me no some inconsistencies in the layout, such as some words not being capitalised. Users also missed the lack of pictures and would like the odometer input in the value car feature of the app to be in kilometres rather than miles. This is an easy fix as I can red in kilometres. 